# -*- coding: utf-8 -*-
"""ECGR_4106_HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uSelHYCnSY_mxVIhdiM4hqNmvpkLXzVV
"""

from google.colab import drive
drive.mount('/content/drive')
file_path = "drive/MyDrive/ECGR 4106/HW_2/"

"""Problem 1"""

import torch
import numpy as np 
import pandas as pd 

# load the dataset
housing = pd.DataFrame(pd.read_csv(file_path + "Housing.csv")) 

# num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price'] 
num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'price']

varList=['mainroad','guestroom','basement','hotwaterheating','airconditioning', 'prefarea']

# Defining the map function
def binary_map(x):
    return x.map({'yes':1,"no":0})

housing[varList] = housing[varList].apply(binary_map)

# splitting the data
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

scaler = StandardScaler()
# scaler = MinMaxScaler()

np.random.seed(0)
df_train, df_test = train_test_split(housing, train_size=0.8, test_size=0.2, random_state=np.random)

df_Newtrain = df_train[num_vars]
df_Newtest = df_test[num_vars]

# scaling the data
df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])
df_Newtest[num_vars] = scaler.fit_transform(df_Newtest[num_vars])

y_Newtrain = df_Newtrain.pop('price')
x_Newtrain = df_Newtrain
y_Newtest = df_Newtest.pop('price')
x_Newtest = df_Newtest

# convert the data to tensors
x_train = torch.tensor(x_Newtrain.values).float()
x_val = torch.tensor(x_Newtest.values).float()
y_train = torch.tensor(y_Newtrain.values).float().unsqueeze(-1)
y_val = torch.tensor(y_Newtest.values).float().unsqueeze(-1)
print(y_val.size())

def training_loop(n_epochs, optimizer, model, loss_fn, x_train, x_val, y_train, y_val):
    for epoch in range(1, n_epochs + 1):
        p_train = model(x_train) # <1>
        loss_train = loss_fn(p_train, y_train)

        p_val = model(x_val) # <1>
        loss_val = loss_fn(p_val, y_val)
        
        optimizer.zero_grad()
        loss_train.backward() # <2>
        optimizer.step()

        if epoch == 1 or epoch % 10 == 0:
            print(f"Epoch {epoch}, Training loss {loss_train.item():.4f},"
                  f" Validation loss {loss_val.item():.4f}")

import torch.optim as optim
import torch.nn as nn

seq_model = nn.Sequential(
            nn.Linear(len(num_vars)-1, 8),
            nn.Tanh(),
            nn.Linear(8, 1))

optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)

training_loop(
    n_epochs = 200, 
    optimizer = optimizer,
    model = seq_model,
    loss_fn = nn.MSELoss(),
    x_train = x_train,
    x_val = x_val, 
    y_train = y_train,
    y_val = y_val)

seq_model_2 = nn.Sequential(
            nn.Linear(len(num_vars)-1, 8),
            nn.Tanh(),
            nn.Linear(8, 6),
            nn.Tanh(),
            nn.Linear(6, 4),
            nn.Tanh(),
            nn.Linear(4, 1))

optimizer = optim.SGD(seq_model_2.parameters(), lr=1e-3)

training_loop(
    n_epochs = 200, 
    optimizer = optimizer,
    model = seq_model_2,
    loss_fn = nn.MSELoss(),
    x_train = x_train,
    x_val = x_val, 
    y_train = y_train,
    y_val = y_val)

"""Problem 2"""

class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']

from torchvision import datasets, transforms
data_path = '../data-unversioned/p1ch7/'
cifar10 = datasets.CIFAR10(
    data_path, train=True, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4915, 0.4823, 0.4468),
                             (0.2470, 0.2435, 0.2616))
    ]))

cifar10_val = datasets.CIFAR10(
    data_path, train=False, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4915, 0.4823, 0.4468),
                             (0.2470, 0.2435, 0.2616))
    ]))

cifar10_train = [(img, label) for img, label in cifar10]
cifar10_test = [(img, label) for img, label in cifar10_val]

device = torch.device(
    "cuda") if torch.cuda.is_available() else torch.device("cpu")
print(device)

train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)

model_cifar = nn.Sequential(
            nn.Linear(3072, 512),
            nn.Tanh(),
            nn.Linear(512, 10))

model_cifar.to(device)

learning_rate = 1e-2

optimizer = optim.SGD(model_cifar.parameters(), lr=learning_rate)

loss_fn = nn.CrossEntropyLoss()

n_epochs = 200

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model_cifar(imgs.view(imgs.shape[0], -1))
        loss = loss_fn(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if epoch % 10 == 0:
      print("Epoch: %d, Loss: %f" % (epoch, float(loss)))

train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model_cifar(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Train Accuracy: %f" % (correct / total))

val_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in val_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model_cifar(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Val Accuracy: %f" % (correct / total))

train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)

model_cifar_2 = nn.Sequential(
            nn.Linear(3072, 512),
            nn.Tanh(),
            nn.Linear(512, 256),
            nn.Tanh(),
            nn.Linear(256, 128),
            nn.Tanh(),
            nn.Linear(128, 10))

model_cifar_2.to(device)

learning_rate = 1e-2

optimizer = optim.SGD(model_cifar_2.parameters(), lr=learning_rate)

loss_fn = nn.CrossEntropyLoss()

n_epochs = 200

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))
        loss = loss_fn(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if epoch % 10 == 0:
      print("Epoch: %d, Loss: %f" % (epoch, float(loss)))

train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Train Accuracy: %f" % (correct / total))

val_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in val_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Val Accuracy: %f" % (correct / total))